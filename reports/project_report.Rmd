---
title: "A/B Testing for Email Fundraising"
author: "Greg Tozzi, Max Ziff, and Taeil Goh"
fontsize: 12pt
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: github_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(
    inputFile, encoding = encoding,
    output_format = c('pdf_document'))
    })
---

```{r setup, include=FALSE}

library(data.table)
library(foreign)
library(lmtest)
library(sandwich)
library(stargazer)
library(data.table)
library(knitr)
library(magrittr)

# opts_knit$set(root.dir=normalizePath('../'))
opts_chunk$set(fig.path = "figures/")
options(digits=3)

knitr::opts_chunk$set(echo = FALSE, warning=FALSE, results='asis')

```

# Executive Summary and Recommendations

Thank you for the opportunity to work with the Children's Science Center.  Below you will find a summary of our findings and recommendations. Details of our analysis are included in the technical report that follows this summary.

### Research Questions

We sought to answer two questions posed by the Center. Both questions were related to the effect on opening and click-through rates of treatments in emails sent by the Center soliciting donations ahead of the planned Fall 2020 fundraising surge.

1. Is there a difference in email opening or click-through caused by the choice of using the Executive Director’s name and title in the from line versus using the Board Chair’s name and title?  *While we did observe a difference in opening rates favoring emails from Jill McNabb, the difference was not significant.*
2. Is there a difference in email opening or click-through behavior caused by the choice of using one of two potential subject lines?  *The subject line, “You can be a Catalyst for STEM Learning,” caused significantly higher opening rates.*
3. Are combinations of the two treatments particularly effective?  *We found no significant effect due to the combination of treatments.*

*We did not observe enough clicks to draw conclusions about the effect of the subject and from line choices on click-through rate.*

### Review of the Methodology

The Center provided our team with a list of approximately 12,000 potential subjects for this study. This list was compiled from the Center’s Altru database but specifically excluded individuals who were previously selected by the Center to receive a solicitation by traditional mail. We randomly selected 1,980 subjects to receive an email soliciting a donation. These individuals were evenly split into four groups of 495 with each group being assigned a unique combination of from and subject lines. The 68% of the population that had mailing addresses on file with the Center also had metadata provided by Boodle.ai related to their predicted affinity for various causes. We used this metadata to check the balance of our random assignment

The Center provided a final quality assurance check of the individuals we assigned to the study and removed 23 records.  The Center sent emails to the remaining individuals using Mailchimp on October 20, 2020.  We extracted the results from Mailchimp on October 28, 2020.


### Results

We computed estimates of the effects of each of the two treatments.  These are presented below.  *Significance* in this context refers to our ability to state that an effect is provably different than zero.  Our ability to find significant results was affected by the sample size that we used for this experiment.  We provide details of our analysis in the attached technical appendix.

| Treatment                                        | Effect (confidence interval)     |
|--------------------------------------------------|----------------------------------|
| Subj: You can be a Catalyst for STEM Learning    | [0.004, 0.113] - Significant     |
| From: Nene Spivy, Executive Director             | [-0.064, 0.04] - Not significant |

We did not find a significant effect resulting from the combination of the two treatments.

### Applying the results
We caution against drawing broad conclusions based on this experiment, particularly with respect to questions that the study was not designed to answer.  

We are confident that the method we employed in this case produced evidence that the difference in opening rates was caused by the difference in subject lines.  You can reasonably expect that this result would generalize to the remainder of the candidate email recipients during the current year-end fund drive.  When considering how to apply these findings, please keep the following points in mind:

1. Responsiveness to the two subject lines may have been affected by factors that we could not control for, including the current public health and political situations.
2. We drew our subjects from a filtered list of potential recipients.  Adding individuals to the population of recipients may invalidate the study’s results.

The difference in opening rates between emails sent from Nene Spivy and Jill McNabb is interesting, but it is not significant.  With the sample size we used, our experiment would not have detected a difference in opening rates smaller than roughly 5.5 percentage points.  This is a fairly large difference.  If you are interested in pursuing the question of which principal drives higher opening rates, we suggest that you run another experiment with a larger sample size.

A natural follow-on qustion may be whether there is a difference in open rates between emails sent from a principal and those sent from an organizational email account.  If you would like to consider this, or if you would benefit from a discussion on setting up a testing program, we are happy to set a call.

\newpage

# Technical Appendix

## Introduction and Context

### The Children’s Science Center

The Children’s Science Center (“the Center”) is a non-profit organization headquartered in Fairfax, Virginia.  Founded in 2004, the Center’s principal objective is to build the Northern Virginia region’s first interactive science center at a greenfield location on donated land in Reston, Virginia.  The capital campaign to raise fund to build this facility is the Center’s primary line of effort.  A recent partnership with the Commonwealth of Virginia’s 

Recognizing the long-term nature of the capital campaign and the need to build interest and advocacy, the Center launched a series of intermediate efforts beginning in 2010.  The Center’s first outreach effort was the Museum without Walls, a program that delivered science-based programs to schools around the region.  In 2015, the Center launched the Lab, a test facility located in a shopping mall sited 12 miles outside of the Washington, DC Beltway.  The Center served approximately 70,000 guests per year until the Lab was temporarily shut down in response to the COVID-19 pandemic.

The Center funds operating expenses with Lab admissions, grants, and philanthropic giving.  The Center’s Development Department is responsible for developing and managing the latter two revenue streams.

### The Fundraising Surge

The Center conducts a fundraising surge toward the end of the calendar year, consistent with the charitable giving cycle in the United States.  The organization expected to engage the majority of its stakeholders by email during the surge.  The Development Director expected that several rounds of engagement would be necessary to achieve a conversion in the form of a charitable gift.  Within each individual round, the path to conversion proceeded from receipt to opening to click-through using an embedded link or button.

The Center introduced two major changes to its process ahead of the 2020 fundraising surge.  In previous year-end campaigns, the Center had used Constant Contact (www.constantcontact.com) to send branded emails to stakeholders.  In the 2020 campaign, the Center chose to switch to Mailchimp’s (www.mailchimp.com) free tier.  The Center also engaged a consulting firm that applies machine learning to the nonprofit space to predict top prospects from among non-profit organizations' contact lists.  The firm, Boodle.ai, predicted affinities for aspects of the Center's mission.  Boodle's model requires a physical address as an entering argument, and it was, therefore, only applied to that subset of the Center’s contact list for which physical addresses were entered.  Based on Boodle's findings, the Center chose to engage a subset of their stakeholder list with physical mail and to wall this cohort off from subsequent email engagement.

### Research Questions

The Center's development staff was most strongly interested in conducting experiments to maximize the opening rates of the Center's fundraising emails.  The Development Director and her staff were keenly aware that email opening was the critical first step in achieving conversion.  The follow-on actions---click through and conversion---were areas of secondary interest for this study.  With this in mind, we sought to answer two specific questions posed by the Center.

1. Is there a difference in email opening or click-through caused by using the Executive Director’s name and title in the from line of the solicitation email versus using the Board Chair’s name and title?

2. Is there a difference in email opening or click-through behavior caused by the choice of using one of two potential subject lines?  The two subject lines considered were, “You can be a Catalyst for STEM Learning," and , "Invest in the Power of STEM Learning."

3. Are combinations of the two treatments particularly effective?

## Research Proposal 

### Research Hypotheses

We claim expertise neither in non-profit email fundraising nor in the preferences of the Center's stakeholders.  The Center's development team believed that there would be a significant difference in the opening rates associated with the two email from lines considered, though the staff did not have a sense of which would result in significantly higher opening rates.  Underpinning the staff's expectation that we would find a significant difference was their belief that the Center's stakeholders had a meaningful sense of the Center's leadership and governance structure and would react differently when presented with emails from either the Executive Director or the Board Chair.

Both of the subject lines considered were written by the Center's development staff.  The staff believed that subject lines could result in significantly different opening rates but did not have a going-in belief of which subject line would cause a stronger response.

\newpage
### Experimental Design and Treatment in Details 

We addressed the research questions by construction a 2x2 factorial design using the difference in means estimator with the subjects assigned to one of four balanced groups as shown in the table below.

| Group | Treatments | Design |
|-------|------------|--------|
| 1     | Executive Director; You can be a Catalyst for STEM Learning | $RX_{1,1}O$ |
| 2     | Board Chair; You can be a Catalyst for STEM Learning        | $RX_{2,1}O$ |
| 3     | Executive Director; Invest in the Power of STEM Learning    | $RX_{1,2}O$ |
| 4     | Board Chair; Invest in the Power of STEM Learning           | $RX_{2,2}O$ |

While we are principally interested in the discrete effect of each treatment, this design allows us to explore heterogeneous treatment effects.  Each group would receive a tailored email with assigned from- and subject-line treatments sent through Mailchimp. We tracked opening and click through using Mailchimp out-of-the-box analytics.  More detail about the implementation of the study is provided in the Outcome Measures section below.

### Statistical Power 

The Center's experience across all of its email-delivered messaging suggested that we should expect opening rates on the order of 10%.  To understand what deviations from this expectation would yield, we considered low, medium, and high baseline opening rates between 5% and 25% in our power calculations.

Powers between 0.8 and 0.9 are standard in clinical trials.  We chose 0.85 as a target power and computed minimum detected differences for our range of baseline rates as shown in the figure that follows.  Entering into the experiment, we believed that it would be unlikely that either treatment would produce differences in means as large as those that the power calculations suggested that we would need to report a significant finding.


```{r}
source('../src/helper_functions.R')

baseline_open_rates <- c(0.05, 0.1, 0.15, 0.2, 0.25)
minDiffDetected <- sapply(baseline_open_rates,
                          function(i) power.prop.test(n = 990,
                                                      p1 = i,
                                                      p2 = NULL,
                                                      power = 0.85)$p2 - i)

plot(minDiffDetected ~ baseline_open_rates, axes=F, xlab="baseline open rate", ylab="ATE", pch=16, type="b")
axis(1, at=baseline_open_rates, label=baseline_open_rates, tick=F)
axis(2, at=seq(0.035, 0.055, length.out=3),
     label=seq(0.035, 0.055, length.out=3), tick=F, las=1)
text(0.25, 0.0375,"Minimum difference detected\nas a function of the baseline\nopen rate for power = 0.85", adj=1)
```


### Enrollment Process & Criteria for Subjects

The Center's donor management database contains over 41,000 individual entries, roughly half which include physical addresses. As descrived above, entries that include a physical address have metadata generated Boodle.ai. These metadata predict affinities for causes central to the Center's mission, such as children’s causes, educational causes, cultural causes, and scientific education.  The Center provided us a file listing 12,004 individuals that Center intended to target by email during its Fall 2020 fundraising surge. These individuals represent all of the database entries that include email addresses less specifically excluded individuals (board members, minor children, etc.) and less the cohort of individuals that the center chose to target with a physical mail campaign on Boodle's recommendation.

The Center provided three data files that we used to develop our randomization.  These files are located in the `data` directory in the project repository (www.github.com/gregtozzi/w241_project).

- `anonymized_altru.csv` contains donor data for the Center's entire database of over 41,000 individuals.  Entries are keyed to a unique donor identification number.
- `anonymized_mail.csv` contains entries for a subset of the complete database for which the Center has physical addresses on file.  This file includes third-party generated predictions of affinities for germane causes.  Entries are also keyed to a unique donor identification number.
- `anonymized_index.csv` contains the unique donor identification numbers for the approximately 12,004 individuals that the Center intended to target by email during the Fall 2020 fundraising surge.

We agreed to target a random sample of 1,980 individuals for this experiment.  The intent was to remain within the bounds of the Mailchimp free tier while leaving a small margin for the Center's staff to send test emails to individuals outside of the experiment including to key members of the Center's staff and our group. The Center delivered one of four solicitation emails via Mailchimp’s web interface.

To select subjects, we first filtered the extract of the Center's complete donor database on the list of individuals the Center intended to target by email.  In the process, we removed three duplicated entries.


```{r, echo=FALSE}

# Load the data and subset on IDs of interest
subject_data <- read_subjects('../data/raw/anonymized_altru.csv')
index_ids    <- fread('../data/raw/anonymized_index.csv')
subject_data <- subject_data[lookup_id %in% index_ids$`LOOKUP ID`]

# De-duplicate
duplicates      <- subject_data[ , .(which(.N > 1)), keyby = lookup_id]$lookup_id
duplicate_index <- which(subject_data$lookup_id %in% duplicates)
subject_index   <- setdiff(1:nrow(subject_data), duplicate_index[c(1, 3, 5)])
subject_data    <- subject_data[subject_index , ]
```

We had hoped to incorporate donor history into our randomization scheme and balance checks, but the data provided by the Center was extremely sparse and contained obvious errors.  For every entry in the total gift amount column was either zero or `NA` and `r subject_data[first_gift_amt > 0 , .N]` entries in the first gift amount column were nonzero.  The Center's staff was well aware of the data quality issues in their donor database.  With zero covariates in the provided data, we turned to the output of the third party study contained in `anonymized_mail.csv`.  We joined the data in that set to our existing table.

```{r}
subject_boodle <- read_subjects_mail('../data/raw/anonymized_mail.csv')
subject_boodle <- subject_boodle[lookup_id %in% index_ids$`LOOKUP ID`]
merged_dt      <- merge(subject_data, subject_boodle, all = TRUE)

# De-duplicate (again)
duplicates_merged <- merged_dt[ , .(which(.N > 1)), keyby = lookup_id]$lookup_id
duplicate_index_merged <- which(merged_dt$lookup_id %in% duplicates_merged)
merged_index <- setdiff(1:nrow(merged_dt), duplicate_index_merged[c(1, 3)])
merged_dt <- merged_dt[merged_index , ]
```

As explained above, outputs of the third party study only existed for individuals with mailing addresses on file.  Only `r sum(!is.na(merged_dt$affinity_children))` of the `r nrow(merged_dt)` individuals in the population had data from the third party study as a result.  We discuss the implications to covariate balance checks in the following section.

We conducted our randomization in two steps.  First, we identified 1,980 individuals who would receive an email.  Then, we randomly divided those individuals into four treatment groups of 495 individuals each.


```{r}
set.seed(1505)
merged_dt[ , receive_email := as.integer(sample(.N) <= 1980)]
merged_dt[receive_email == 1, treatment := sample(rep(1:4, .N / 4))]
random_assignment <- merged_dt[receive_email == 1, c("lookup_id", "treatment")]
kable(head(random_assignment), caption = "Examples of random assignment",)
```


### Validation of Randomization Procedure

Data quality issues limited the number of covariates available to perform balance checks.  In the end, we had membership data for our entire sample.  As described above, we also had predicted affinities provided by Boodle.ai for about two-thirds of the sample. These affinities were for elements of the Center's mission:  children's causes, cultural causes, educational causes, and science causes.  The remaining third of the sample did not have mailing addresses on file.  Not having an address on file is an indicator of engagement with the Center, so we created an indicator variable capturing this covariate  There were, of course, no instances in which an individual without an mailing address on file also had data in any of the Boodle-generated covariates.  Including the no mailing address covariate in the balance check required that we run two separate regressions for each treatment variable---one which captured the Boodle.ai-generated affinities and membership data, and the other which captured the lack of a mailing address and membership data.

The regressions presented below show no significant relationship between the covariates we tested and the treatment variables.  We are confident that our randomization was successful.

```{r}
# Code individual treatments
merged_dt[treatment %in% c(1, 2), subj_catalyst := 1]
merged_dt[treatment %in% c(3, 4), subj_catalyst := 0]
merged_dt[treatment %in% c(1, 3), from_ED := 1]
merged_dt[treatment %in% c(2, 4), from_ED := 0]

# Build a factor for entries without 3rd party affinity data
merged_dt[is.na(affinity_science), missing_affinities := 1]
merged_dt[!is.na(affinity_science), missing_affinities := 0]

balance_lm_catalyst <- merged_dt[receive_email == 1, lm(subj_catalyst ~ factor(affinity_children) + factor(affinity_education) + factor(affinity_science) + factor(affinity_culture) + is_member.x)]

balance_lm_catalyst_mbr <- merged_dt[receive_email == 1, lm(subj_catalyst ~ is_member.x + missing_affinities)]

balance_lm_ED <- merged_dt[receive_email == 1, lm(from_ED ~ factor(affinity_children) + factor(affinity_education) + factor(affinity_science) + factor(affinity_culture) + is_member.x)]

balance_lm_ED_mbr <- merged_dt[receive_email == 1, lm(from_ED ~ is_member.x + missing_affinities)]

balance_catalyst <- calculate_rse_ci(balance_lm_catalyst)
balance_ED <- calculate_rse_ci(balance_lm_ED)
balance_catalyst_mbr <- calculate_rse_ci(balance_lm_catalyst_mbr)
balance_ED_mbr <- calculate_rse_ci(balance_lm_ED_mbr)

stargazer(balance_catalyst, balance_catalyst_mbr, balance_ED, balance_ED_mbr,
          se = c(list(balance_catalyst$rse_), list(balance_catalyst_mbr$rse_), 
                 list(balance_ED$rse_), list(balance_ED_mbr$rse_)), 
          header=FALSE,
          type="latex",
          font.size = "scriptsize", 
          title = "Covariate balance",
          dep.var.labels=c("Subj: Catalyst", "From: ED"),
          covariate.labels=c("Affinity - Children 35", 
                             "Affinity - Children 90",
                             "Affinity - Education 35",
                             "Affinity - Education 90",
                             "Affinity - Science 35",
                             "Affinity - Science 90",
                             "Affinity - Culture 35",
                             "Affinity - Culture 42",
                             "Affinity - Culture 50",
                             "Affinity - Culture 90",
                             "Member",
                             "No address"),
          align=FALSE
          )
```


### Outcome Measures

Implementing the experiment involved finding the proper balance between Mailchimp technical capabilities, the Center’s expertise and availability, and the need to respect the individuals’ privacy on the Center’s mailing lists. Because the Center was new to Mailchimp, it was important to keep the implementation simple and inexpensive. This ruled out the use of Mailchimp’s in-built experimentation features, both from the point of view of expense (experimentation features require paid subscriptions) and transparency: Mailchimp experimentation concealed their own randomization, which, while probably sound, can not be externally audited because it is proprietary. 

The four treatment groups were modeled in Mailchimp by four different “campaigns”, Mailchimp’s basic data object for representing at least one email sent. Each campaign used a separate email template, with the appropriate variations hard-coded. A toy Mailchimp list and template were used to prove out the Mailchimp implementation. Also, in the live implementation, dummy addresses were added to each group that targeted the researchers, so we were reasonably confident that each group correctly received the email according to the experimental design.

Outcomes were gathered using a custom python script that in turn used Mailchimp RESTful API. During development, the script was run against the separate toy implementation to minimize the need to connect to the Center’s live data, thus minimizing the risk of accidental leakage of private data. 

Mailchimp’s API provides a complete history of events for each send in each campaign. In particular, it distinguishes between these event types: open, click, and bounce. Mailchimp records multiple instances of each event type: a recipient may open or click on a link in an email multiple times, and each event is recorded. For our purposes, we gathered only the date-time of each event type’s first occurrence, if any. As one would expect, click events were always preceded by an open event, and bounce events precluded any other events.

The Mailchimp API reports events keyed by email address. Thus it was necessary to run this script carefully, only on a trusted machine, and to then join this raw, sensitive data back to the full data set, with its covariates.

### Accounting for Non-Compliance

We observed two avenues for non-compliance.  In the first, twelve individuals did not receive an email because they were removed from the study after being assigned to a group.  The Center determined that these individuals could not be included in the study because they were minor children or were related to board members.  Given the challenge of maintaining a stakeholder database with inputs across departments, we were very pleased that this form of non-compliance affected well under 1% of our subjects.

The second avenue for non-compliance was outdated or ill-formed email addresses.  Mailchimp reported that 23 of our emails were not delivered.  Again, we were impressed by the overall cleanliness of the data the Center provided.

To account for non-compliance conservatively, we coded the 35 non-compliers as having not opened the email.  However, we believe that an argument could be made for simply neglecting these individuals from the analysis as the group of 12 who were removed should never have been included in the population, and the 23 who bounced would not have received the email in any event.

## Research Report

### Experiment Results 

```{r}
d <- fread("../data/raw/20201029_results_with_covariates")

d[treatment_assigned == 1 | treatment_assigned == 2, subject := 1]
d[treatment_assigned == 3 | treatment_assigned == 4, subject := 0]
d[treatment_assigned == 1 | treatment_assigned == 3, sender := 1]
d[treatment_assigned == 2 | treatment_assigned == 4, sender := 0]

# mark zero if non-compliant
d[is.na(treatment_received), open := 0]
d[is.na(treatment_received), click := 0]

```


```{r}
m1.open = d[ , lm(open ~ subject)]
m2.open = d[ , lm(open ~ sender)]
m3.open = d[ , lm(open ~ subject + sender + subject*sender)]
m1.open = calculate_rse_ci(m1.open)
m2.open = calculate_rse_ci(m2.open)
m3.open = calculate_rse_ci(m3.open)

m4.click = d[ , lm(click ~ subject + sender + subject*sender)]
m4.click = calculate_rse_ci(m4.click)
```

The experimental results are shown formally in the table below.  We first considered each treatment individually in columns 1 and 2.  We consider a complete model of opening rates in column 3.  While we recorded very few clicks, we included a model of clickthrough rates in column 4 for completeness.  We report robust standard errors here and elsewhere.

Column 3 considers both treatments and the possibility of heterogeneous treatment effects.  In comparing this model with those presented in columns 1 and 2, we note the following:

- Considering both treatments and the interaction term increases both the estimate of the effect of the use of the *Catalyst* subject line and its standard error.  The estimate of the effect in the single factor model was [`r m1.open$rse.ci_['subject',]`].  In the saturated model, this shifts and expands to [`r m3.open$rse.ci_['subject',]`].

- We estimate the effect of the use of the *Board Chair* subject line treatment to be [`r m3.open$rse.ci_['sender',]`] in the saturated model.  While not significant, the direction of the effect was not expected by the Center's staff, and this treatment may be worth further study with an appropriately-powered experiment.

- We did not find a significant heterogeneous treatment effect.

```{r}
stargazer(m1.open, m2.open, m3.open, m4.click,
          se = c(list(m1.open$rse_), list(m2.open$rse_), 
                 list(m3.open$rse_), list(m4.click$rse_)), 
          header=FALSE, 
          type='latex',
          font.size = "scriptsize", 
          title = "Effect of different subjects and senders",
          dep.var.labels=c("Open Rate", "Click Rate"),
          covariate.labels=c("Subj - Catalyst", 
                             "From - Exec Dir" 
                             , "Subj - Catalyst:From - Exec Dir", 
                             "Baseline"),
          align=FALSE
          )
```

### Generalizability and Mediation Analysis

Although we saw statistically significant results in the context of our experiment, we are cautious to ascribe specific mediation mechanisms that would allow these results to be generalized. We feel that the specific variations we observed should generalize to the larger population from which we took our random sample. That is, if the Center had sent email to the larger group of 11k using the preferred subject line, we would expect that the improved open rate would apply to the entire population, but it is not clear that even that effect could be expected for hypothetical future mailings. We note that this experiment was run at a very specific moment, right before the Presidential election of 2020. There may have been special effects related to that time: the climate may have made the audience particularly responsive to certain keywords, or even more or less inclined to open mail at all.

The Center’s development personnel have expressed that the lesson they draw from this study is that email with a personal from-line out-performs email with a role from-line (e.g. “Nene Spivy” vs. “Executive Director”). This is interesting because this was not explicitly tested, and it illustrates the difficulty of translating intentions and desires of decision-makers into data science research questions. 

It is interesting to speculate on what would be an interesting experiment or series of experiments to test a general hypothesis such as “email with a personal from-line out-performs email with a role from-line”. Stakeholders' relationships with the Center will change as the Center evolves.  As a result, we expect that the experimental findings would need to be periodically refreshed as the Center's stakeholders grow in size and as the date of the new facility's opening draws nearer.  It seems that ideally, experiments would become an on-going part of a development program.  

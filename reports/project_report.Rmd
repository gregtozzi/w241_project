---
title: "A/B Testing for Email Fundraising"
author: "Greg Tozzi, Max Ziff, and Taeil Goh"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: github_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(
    output_file = 'README.md',
    inputFile, encoding = encoding,
    output_format = c('github_document'))
    })
---

```{r setup, include=FALSE}

library(data.table)
library(foreign)
library(lmtest)
library(sandwich)
library(stargazer)
library(data.table)
library(knitr)
library(magrittr)

# opts_knit$set(root.dir=normalizePath('../'))
opts_chunk$set(fig.path = "figures/")

knitr::opts_chunk$set(echo = FALSE, warning=FALSE)

```

## Executive Summary and Recommendations

Thank you for the opportunity to work with the Children's Science Center.  Below you will find a summary of our findings and recommendations. This report is written for the Children's Science Center. Still, it includes technical details in the second part to validate the work by other data scientists can validate and reproduce the results where our conclusion is based on. Due to the sensitivity of data stored on Mailchimp, it is required to export script by a person at the Children's Science Center who has access to Mailchimp records. Details of our analysis are included in the technical report that follows this summary.

#### Research Questions

We sought to answer two questions posed by the Center.  Both questions were related to the effect on opening and click-through rates of treatments in emails sent by the Center soliciting donations ahead of the planned Fall 2020 fundraising surge.

1. Is there a difference in email opening or click-through caused by the choice of using the Executive Director‚Äôs name and title in the from line versus using the Board Chair‚Äôs name and title?  *While we did observe a difference in opening rates favoring emails from Jill McNabb, the difference was not significant.*
2. Is there a difference in email opening or click-through behavior caused by the choice of using one of two potential subject lines?  *The subject line, ‚ÄúYou can be a Catalyst for STEM Learning üí°,‚Äù caused significantly higher opening rates.*
3. Were combinations of the two treatments particularly effective?  *We found no significant effect due to the combination of treatments.*

*We did not observe enough clicks to draw conclusions about the effect of the subject and from line choices on click-through rate.*

#### Review of the Methodology
The Center provided our team with a list of approximately 12,000 potential subjects for this study.  This list was compiled from the Center‚Äôs Altru database but specifically excluded individuals who were previously selected by the Center to receive a solicitation by traditional mail.  We randomly selected 1,980 subjects to receive an email soliciting a donation.  These individuals were evenly split into four groups of 495 with each group being assigned a unique combination of from and subject lines.  The 68% of the population that had mailing addresses on file with the Center also had metadata provided by Boodle.ai related to their predicted affinity for various causes.  We used this metadata to check the balance of our random assignment

The Center provided a final quality assurance check of the individuals we assigned to the study and removed 23 records.  The Center sent emails to the remaining individuals using Mailchimp on October 20, 2020.  We extracted the results from Mailchimp on October 28, 2020.

#### Results

Results for open rates are summarized below.

| Open Rates (24% avg) | You can be a Catalyst for STEM Learning üí° (26.4%) | Invest in the Power of STEM Learning üí° (21.6%) |
|--|--|--|
|Nene Spivy, Executive Director (22.8%) | 24.4% | 21.2% |
|Jill McNabb, Board Chair (25.2%) | 28.2% | 22.0% |

### Applying the results
We caution against drawing broad conclusions based on this experiment, particularly with respect to questions that the study was not designed to answer.  

We are confident that the method we employed in this case produced evidence that the difference in opening rates was caused by the difference in subject lines.  You can reasonably expect that this result would generalize to the remainder of the candidate email recipients during the current year-end fund drive.  We suggest caution when drawing broader conclusions, however for the following reasons:

1. Responsiveness to the two subject lines may have been affected by factors that we could not control for, including the current public health and political situations.
2. We drew our subjects from a filtered list of potential recipients.  Adding individuals to the population of recipients may invalidate the study‚Äôs results.

The difference in opening rates between emails sent from Nene Spivey and Jill McNabb is interesting, but it is not significant.  With the sample size we used, our experiment would not have detected a difference in opening rates smaller than roughly 5.5 percentage points.  This is a fairly large difference.  If you are interested in pursing the question of which principal drives higher opening rates, we suggest that you run another experiment with a larger sample size. 

If you desire to run additional tests in subsequent emails, or if you would benefit from a discussion on setting up a testing program, we are happy to set a call.

## Introduction and Context 

#### The Children‚Äôs Science Center

The Children‚Äôs Science Center (‚Äúthe Center‚Äù) is a non-profit organization headquartered in Fairfax, Virginia.  Founded in TODO, the Center‚Äôs principal objective is to build the Northern Virginia region‚Äôs first interactive science center at a greenfield location on donated land in Reston, Virginia.  The capital campaign to raise fund to build this facility is the Center‚Äôs primary line of effort.  A recent partnership with the Commonwealth of Virginia‚Äôs 

Recognizing the long-term nature of the capital campaign and the need to build interest and advocacy, the Center launched a series of intermediate efforts beginning in `TODO.`  The Center‚Äôs first outreach effort was the Museum without Walls, a program that delivered science-based programs to schools around the region.  In `TODO`, the Center launched the Lab, a test facility located in a shopping mall sited 12 miles outside of the Washington, DC Beltway.  The Lab hosted approximately `TODO` guests per year until it was temporarily shut down in response to the COVID-19 pandemic.

The Center funds operating expenses with Lab admissions, grants, and philanthropic giving.  The Center‚Äôs Development Department is responsible for developing and managing the latter two revenue streams.

##### The Fundraising Surge

The Center conducts a fundraising surge toward the end of the calendar year, consistent with the charitable giving cycle in the United States [`TODO`: add reference].

The Center expected to engage the majority of its stakeholders by email.  The Center viewed email marketing as repeated iterations of a three step process.  The Development Director expected that several rounds of engagement would be necessary to achieve a conversion in the form of a charitable gift.  Within each individual round, the path to conversion proceeded from receipt to opening to click-through using an embedded link or button.

The Center introduced two major changes to its process ahead of the 2020 fundraising surge.  In previous year-end campaigns, the Center had used Constant Contact to send branded emails to stakeholders.  In the 2020 campaign, the Center chose to switch to Mail Chimp‚Äôs free tier.  The Center also engaged a consulting firm that applies machine learning to the nonprofit space to predict top prospects from among the Center‚Äôs contact list and to predict affinities to aspects of the Center‚Äôs mission.  The consulting firm‚Äôs model requires a physical address as an entering argument, and it was, therefore, only applied to that subset of the Center‚Äôs contact list for which physical addresses were entered.  Based on the consulting firm‚Äôs output, the Center chose to engage a subset of their stakeholder list with physical mail and to wall this cohort off from subsequent email engagement.

#### Research Questions

The Center's development staff was most strongly interested in conducting experiments to maximize the opening rates of the Center's fundraising emails.  The Development Director and her staff were keenly aware that email opening was the critical first step in achieving conversion.  The follow-on actions---click though and conversion---were areas of secondary interest for this study.  With this in mind, we sought to answer two specific questions posed by the Center.

1. Is there a difference in email opening or click-through caused by using the Executive Director‚Äôs name and title in the from line of the solicitation email versus using the Board Chair‚Äôs name and title?

2. Is there a difference in email opening or click-through behavior caused by the choice of using one of two potential subject lines?  The two subject lines considered were, ‚ÄúYou can be a Catalyst for STEM Learning üí°," and , "Invest in the Power of STEM Learning üí°."


## Research Proposal 

#### Research Hypotheses

We claim expertise neither in non-profit email fundraising nor in the preferences of the Center's stakeholders.  The Center's development team believed that there would be a significant difference in the opening rates associated with the two email from lines considered, though the staff did not have a sense of which would result in significantly higher opening rates.  Underpinning the staff's expectation that we would find a significant difference was their belief that the Center's stakeholders had a meaningful sense of the Center's leadership and governance structure and would react differently when presented with emails from either the Executive Director or the Board Chair.

Both of the subject lines considered were written by the Center's development staff.  The staff believed that subject lines could result in significantly different opening rates but did not have a going-in belief of which subject line would cause a stronger response.

#### Experimental Design and Treatment in Details 

`Greg` 

`see page 428`

We addressed the research questions by construction a 2x2 factorial design using the difference in means estimator with the subjects assigned to one of four balanced groups:

- Group 1 - From Executive Director; You can be a Catalyst for STEM Learning üí°
- Group 2 - From Board Chair; You can be a Catalyst for STEM Learning üí°
- Group 3 - From Executive Director; Invest in the Power of STEM Learning üí°
- Group 4 - From Board Chair; Invest in the Power of STEM Learning üí°

While we are principally interested in the discrete effect of each treatment, this design allows us to explore heterogeneous treatment effects.  Each group would received a tailored email containing its assigned treatments sent through Mail Chimp.  We tracked opening and click through using Mail Chimp's out-of-the-box analytics.

#### Statistical Power 

The Center's experience across all of its email-delivered messaging suggested that we should expect opening rates on the order of 10%.  To understand what deviations from this expectation would yield, we considered low, medium, and high baseline opening rates between 5% and 25% in our power calculations.

Powers between 0.8 and 0.9 are standard in clinical trials.  We chose 0.85 as a target power and computed minimum detected differences for our range of baseline rates.  Entering into the experiment, we believed that it would be unlikely that either treatment would produce differences in means as large as those that the power calculations suggested that we would need to report a significant finding.

`Taeil` to add x y label 

```{r}
source('../src/helper_functions.R')

baseline_open_rates <- c(0.05, 0.1, 0.15, 0.2, 0.25)
minDiffDetected <- sapply(baseline_open_rates,
                          function(i) power.prop.test(n = 990,
                                                      p1 = i,
                                                      p2 = NULL,
                                                      power = 0.85)$p2 - i)

plot(minDiffDetected ~ baseline_open_rates, axes=F, xlab="baseline open rate", ylab="ATE", pch=16, type="b")
axis(1, at=baseline_open_rates, label=baseline_open_rates, tick=F)
axis(2, at=seq(0.035, 0.055, length.out=3),
     label=seq(0.035, 0.055, length.out=3), tick=F, las=1)
text(0.25, 0.0375,"Minimum difference detected\nas a function of the baseline\nopen rate for power = 0.85", adj=1)
```


#### Enrollment Process & Criteria for Subjects

The Center's donor managment database contains over 41,000 individual entries, roughly half which include physical addresses.  Entries that include a phyical address have metadata generated by a third party that predict affinities for causes central to the Center's mission.  The Center provided us a file listing 12,004 individuals that Center intended to target by email during its Fall 2020 fundraising surge.  These individuals represent all of the database entries that include email addresses less specifically excluded individuals (board members, minor children, etc.) and less a special cohort that the Center intended to target through a physical mail campaign based on the third party's predicted capacity and willingness to donate.

The Center provided three data files that we used to develop our randomization.  These files are located in the `data` directory in the project repostiory.

1. `anonymized_altru.csv` contains donor data for the Center's entire database of over 41,000 individuals.  Entries are keyed to a unique donor idnetificaiton number.
2. `anonymized_mail.csv` contains entries for a subset of the complete database for which the Center has physical addresses on file.  This file includes third-party generated predictions of affinities for germane causes.  Entries are also keyed to a unique donor idnetificaiton number.
3. `anonymized_index.csv` contains the unique donor identification numbers for the approximately 12,004 individuals that the Center intended to target by email during the Fall 2020 fundraising surge.

We agreed to target a random sample of 1,980 individuals for this experiment.  The intent was to remain within the bounds of the Mail Chimp free tier while leaving a small margin for the Center's staff to send test emails to individuals outside of the experiment including to key members of the Center's staff and our group. The Center delivered one of four solicitation emails via Mail Chimp's web interface.

To select subjects, we first filtered the extract of the Center's complete donor database on the list of individuals the Centered intended to target by email.  In the process, we removed three duplicated entries.

```{r, echo=FALSE}

# Load the data and subset on IDs of interest
subject_data <- read_subjects('../data/raw/anonymized_altru.csv')
index_ids    <- fread('../data/raw/anonymized_index.csv')
subject_data <- subject_data[lookup_id %in% index_ids$`LOOKUP ID`]

# De-duplicate
duplicates      <- subject_data[ , .(which(.N > 1)), keyby = lookup_id]$lookup_id
duplicate_index <- which(subject_data$lookup_id %in% duplicates)
subject_index   <- setdiff(1:nrow(subject_data), duplicate_index[c(1, 3, 5)])
subject_data    <- subject_data[subject_index , ]
```

We had hoped to incorporate donor history into our randomization scheme and balance checks, but the data provided by the Center was extremely sparse and contained obvious errors.  For every entry in the total gift amount column was either zero or `NA` and `r subject_data[first_gift_amt > 0 , .N]` entries in the first gift amount column were nonzero.  The Center's staff was well aware of the data quality issues in their donor database.  With zero covariates in the provided data, we turned to the output of the third party study contained in `anonymized_mail.csv`.  We joined the data in that set to our existing table.

```{r}
subject_boodle <- read_subjects_mail('../data/raw/anonymized_mail.csv')
subject_boodle <- subject_boodle[lookup_id %in% index_ids$`LOOKUP ID`]
merged_dt      <- merge(subject_data, subject_boodle, all = TRUE)

# De-duplicate (again)
duplicates_merged <- merged_dt[ , .(which(.N > 1)), keyby = lookup_id]$lookup_id
duplicate_index_merged <- which(merged_dt$lookup_id %in% duplicates_merged)
merged_index <- setdiff(1:nrow(merged_dt), duplicate_index_merged[c(1, 3)])
merged_dt <- merged_dt[merged_index , ]
```


As explained above, outputs of the third party study only existed for individuals with mailing addresses on file.  Only `r sum(!is.na(merged_dt$affinity_children))` of the `r nrow(merged_dt)` individuals in the population had data from the third party study as a result.  We discuss the implications to covariate balance checks in a following section.

We conducted our randomization in two steps.  First, we identified 1,980 individuals who would receive an email.  Then, we randomly divided those individuals into four treatment groups of 495 individuals each.

```{r}
set.seed(1505)
merged_dt[ , receive_email := as.integer(sample(.N) <= 1980)]
merged_dt[receive_email == 1, treatment := sample(rep(1:4, .N / 4))]
random_assignment <- merged_dt[receive_email == 1, c("lookup_id", "treatment")]
random_assignment %>% head
```


#### Validation of Randomization Procedure

Data quality issues limited the number of covariates available to perform balance checks.  In the end, we had membership data for our entire sample.  We also had predicted affinities provided by Boodle.ai for about two-thirds of the sample.  These affinities were for elements of the Center's mission:  children's causes, cultural causes, educational causes, and science causes.  Boodle's methodology requires a mailing address, so the remaining third of the sample did not have mailing addresses on file.  Not have an address on file is an indicator of engagement with the Center, so we created an indicator variable capturing this covariate  There were, of course, no instances in which an individual without an mailing address on file also had data in any of the Boodle-generated covariates.  Including the no mailing address covariate in the balance check required that we run two separate regressions for each treatment variable---one which captured the Boodle.ai-generated affinities and membership data, and the other which captured the lack of a mailing address and membership data.

The regressions show no significant relationship between the covariates we tested and the treatment variables.  We are confident that our randomization was successful.

```{r}
# Code individual treatments
merged_dt[treatment %in% c(1, 2), subj_catalyst := 1]
merged_dt[treatment %in% c(3, 4), subj_catalyst := 0]
merged_dt[treatment %in% c(1, 3), from_ED := 1]
merged_dt[treatment %in% c(2, 4), from_ED := 0]

# Build a factor for entries without 3rd party affinity data
merged_dt[is.na(affinity_science), missing_affinities := 1]
merged_dt[!is.na(affinity_science), missing_affinities := 0]

balance_lm_catalyst <- merged_dt[receive_email == 1, lm(subj_catalyst ~ factor(affinity_children) + factor(affinity_education) + factor(affinity_science) + factor(affinity_culture) + is_member.x)]

balance_lm_catalyst_mbr <- merged_dt[receive_email == 1, lm(subj_catalyst ~ is_member.x + missing_affinities)]

balance_lm_ED <- merged_dt[receive_email == 1, lm(from_ED ~ factor(affinity_children) + factor(affinity_education) + factor(affinity_science) + factor(affinity_culture) + is_member.x)]

balance_lm_ED_mbr <- merged_dt[receive_email == 1, lm(from_ED ~ is_member.x + missing_affinities)]

balance_catalyst <- calculate_rse_ci(balance_lm_catalyst, merged_dt)
balance_ED <- calculate_rse_ci(balance_lm_ED, merged_dt)
balance_catalyst_mbr <- calculate_rse_ci(balance_lm_catalyst_mbr, merged_dt)
balance_ED_mbr <- calculate_rse_ci(balance_lm_ED_mbr, merged_dt)

stargazer(balance_catalyst, balance_catalyst_mbr, balance_ED, balance_ED_mbr,
          se = c(list(balance_catalyst$rse_), list(balance_catalyst_mbr$rse_), 
                 list(balance_ED$rse_), list(balance_ED_mbr$rse_)), 
          type="text",
          title = "Covariate Balance",
          dep.var.labels=c("Subj: Catalyst", "From: ED"),
          covariate.labels=c("Affinity - Children 35", 
                             "Affinity - Children 90",
                             "Affinity - Education 35",
                             "Affinity - Education 90",
                             "Affinity - Science 35",
                             "Affinity - Science 90",
                             "Affinity - Culture 35",
                             "Affinity - Culture 42",
                             "Affinity - Culture 50",
                             "Affinity - Culture 90",
                             "Member",
                             "No address"),
          align=FALSE
          )
```


#### Outcome Measures

Implementing the experiment involved finding the proper balance between MailChimp's technical capabilities, the Center's expertise and availability, and the need to respect the privacy of the individuals on the Center's mailing lists. Because the Center was new to MailChimp, it was important to keep the implementation simple and inexpensive. This ruled out the use of MailChimp's in-built experimentation features, both from the point of view of expense (experimentation features require paid subscriptions) and transparency: MailChimp's experimentation concealed their own randomization which, while probably sound, can not be externally audited because it is proprietary. 

The four treatment groups were modeled in MailChimp by four different "campaigns", MailChimp's basic data object for representing at least one email send. Each campaign used a separate email tenmplate, with the appropriate variations hard-coded. A toy MailChimp list and template we==re used to prove out the MailChimp implementation. In addition, in the live implementation, dummy addresses were added to each group that targeted the researchers, so we were reasonably confident that each group correctly received the email according to the experimental design.

Outcomes were gathered using a custom python script that in turn used MailChimp's rest-ful API. During development, the script was run against the separate, toy implementation, so as to minimize the need to connect to the Center's live data, and thus minimize the risk of accidental leakage of private data. 

MailChimp's API provides a complete history of events for each send in each campaign. In particular, it distinguishes between these event types: open, click and bounce. MailChimp records multiple instances of each event type: a recipient may open or click on a link in an email multiple times, and each event is recorded. For our purposes, we gathered only the date-time of the first occurrence of each event type, if any. As one would expect, click events were always preceded by an open event, and bounce events precluded the occurrence of any other events.

The MailChimp API reports events keyed by email address, thus it was necessary to run this script carefully, only on a trusted machine, and to then join this raw, sensitive data back to the full data set, with its covariates.


#### Plan to analyze the data

- how you plan to analyze the data 

`Max`

we have waited for x days .... 


#### Accounting for Non-Compliance

- `Greg`  

We considered non-compliers (all recipients who we were not able to deliver the emails) marked them that they would not receive nor click the email. 


## Research Report


#### Experiment Results 


```{r}
d <- fread("../data/raw/20201029_results_with_covariates")

d[treatment_assigned == 1 | treatment_assigned == 2, subject := 1]
d[treatment_assigned == 3 | treatment_assigned == 4, subject := 0]
d[treatment_assigned == 1 | treatment_assigned == 3, sender := 1]
d[treatment_assigned == 2 | treatment_assigned == 4, sender := 0]

# mark zero if non-compliant. Well, can we? 
d[is.na(treatment_received), open := 0]
d[is.na(treatment_received), click := 0]

```


```{r}
m1.open = d[ , lm(open ~ subject)]
m2.open = d[ , lm(open ~ sender)]
m3.open = d[ , lm(open ~ subject + sender + subject*sender)]
m1.open = calculate_rse_ci(m1.open, d)
m2.open = calculate_rse_ci(m2.open, d)
m3.open = calculate_rse_ci(m3.open, d)

m4.click = d[ , lm(click ~ subject + sender + subject*sender)]
m4.click = calculate_rse_ci(m4.click, d)
```

`Max` 

- Attrition 
- additional outcome (e.g., unsubscription rate)
- Mediation analysis: why more open rate? 
- description of experimental results 

We are confident that the method we employed in this case produced evidence that the difference in opening rates was caused by the difference in subject lines.  You can reasonably expect that this result would generalize to the remainder of the candidate email recipients during the current year-end fund drive.  We suggest caution when drawing broader conclusions, however for the following reasons:

1. Responsiveness to the two subject lines may have been affected by factors that we could not control for, including the current public health and political situations.

2. We drew our subjects from a filtered list of potential recipients.  Adding individuals to the population of recipients may invalidate the study‚Äôs results.

Meanwhile, we did not observe enough clicks to draw conclusions about the effect of the subject and from line choices on click-through rate.The following table describes statistic details how we analyzed the experiment. 

- Subject: ***`r round(m3.open$coefficients[[2]]*100,2)`*** percentage point [`r round(m3.open$rse.ci_[2,1]*100,1)`pp ~ `r round(m3.open$rse.ci_[2,2]*100,1)`pp]
- Sender: no significant difference


```{r}
stargazer(m1.open, m2.open, m3.open, m4.click,
          se = c(list(m1.open$rse_), list(m2.open$rse_), 
                 list(m3.open$rse_), list(m4.click$rse_)), 
          type="text",
          title = "Effect of different subjects and senders",
          dep.var.labels=c("Open Rate", "Click Rate"),
          covariate.labels=c("Subject - Catalyst", 
                             "From - Board Chair" 
                             , "Subject - Catalyst, from - Chair", 
                             "Subject - Invest, from - Director"),
          align=TRUE
          )
```

